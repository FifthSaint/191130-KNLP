# 텍스트의 기초통계

이 장에선 파이썬에서 텍스트를 데이터로서 어떻게 살펴보는지 기초를 연마합니다. 
우선 nltk를 활용해서 용법과 친숙해 진 뒤 한글에 대해서 살펴보겠습니다.
데이터를 마련해 봅니다.


```python
import nltk
```


```python
nltk.download()
```


```python
from nltk.book import *
```


```python
데이터가 어떤 형태인지 수시로 쓰이는 방법
```


```python
type(text1)
```


```python
nltk에는 편리하게 마련된 메소드들이 있습니다.
메소드에 대해 깊이 생각할 필요 없이 그냥 쳐봅시다.
```


```python
text1.concordance("monstrous")
```


```python
text1.similar("monstrous")
```


```python
text2.similar("monstrous")
```


```python
text2.common_contexts(["monstrous", "very"])
```

시각화는 텍스트 데이터를 살펴볼 때도 흥미로운 방법


```python
import matplotlib
text4.dispersion_plot(["citizens", "democracy", "freedom", "duties", "Amercia"])
```

기사 쓰는 로봇처럼..


```python
text3.generate()
```

기본적인 숫자 통계, 길이


```python
len(text3)
```

중복을 제거하는 방법


```python
unique_text3 = set(text3)
```


```python
len(unique_text3)
```

이를 활용하면 쉽게 어떤 텍스트가 얼마나 어휘의 다양성을 가지고 있는지 알아볼 수 있습니다


```python
len(unique_text3) / len(text3)
```

특정 단어에 집중해서 몇 개가 나왔는지, 전체 단어 중에서 비율은 얼마나 되는지 등을 계산


```python
text3.count("smote")
100 * text4.count('a') / len(text4)
```

다양성을 하나의 함수로 정의합니다
그러면 활용하기 쉽죠


```python
def lexical_diversity(text):
    return len(set(text)) / len(text)

def percentage(count, total):
    return 100 * count / total
```


```python
lexical_diversity(text3)
```


```python
lexical_diversity(text5)
```


```python
percentage(4,5)
percentage(text4.count('a'), len(text4))
```

그러면 한글에 적용해 봅시다.
헌법 데이터 활용

```python
from konlpy.corpus import kolaw
```


```python
fids = kolaw.fileids()
fobj = kolaw.open(fids[0])
print(fobj.read(150))
kotext = fobj.read()
```


```python
len(kotext)
```

역시 다양성을 체크해 보려고 하는데...


```python
sorted(set(kotext))
```


```python
len(set(kotext))
```

단어별로 추출하지 못하고 '문자'별로 추출하기 때문

각 단어별로 나누는 기능이 필요. 
이것이 토크나이저(tokenizer)입니다.


```python
from konlpy.tag import Okt
okt=Okt()
print(okt.morphs(kotext))
```


```python
kotkn = okt.morphs(kotext)
len(kotkn)
```

이제 원했던 다양성을 계산해 보면...


```python
len(set(kotkn)) / len(kotkn)
```

이 지점에서 *전처리*에 대해...
### [텍스트 전처리](https://wikidocs.net/21694)

텍스트를 살펴보는 유용한 방법중에 하나는 어떤 단어들로 이루어져 있는지를 살펴 보는 것. 
단어의 통계.
다시 nltk로 돌아가서... 미리 정의된 방법을 활용해 살펴보겠습니다.


```python
fdist1 = FreqDist(text1)
fdist1.most_common(50)
```


```python
fdist1['whale']
```

시각화


```python
fdist1.plot(50, cumulative=True)
```

한글에 대해서 살펴본다면? 
이번에는 konlpy의 의안 데이터(kobill)를 활용. 
(이 부분 레퍼런스는 lucypark님의
https://www.lucypark.kr/courses/2015-dm/text-mining.html )


```python
from konlpy.corpus import kobill
files_ko = kobill.fileids()
doc_ko = kobill.open('1809890.txt').read()
```

토큰화를 시켜야


```python
tokens_ko = okt.morphs(doc_ko)
len(tokens_ko)
```


```python
ko = nltk.Text(tokens_ko, name='대한민국 국회 의안 제 1809890호')
ko.vocab()
```

한글 폰트 설정


```python
from matplotlib import rc, font_manager
rc('font', family='NanumGothic')
```


```python
ko.plot(50)
```

그림 크기 키우기


```python
import matplotlib.pyplot as plt

plt.rcParams["figure.figsize"] = (14,4)
```

의안도 그렇고, 모비딕도 그렇고
보시다시피 큰 정보를 주지 못하는 단어들이 많음.
좀 더 모비딕의 특성을 보여주는 단어를 살펴보는 방법은 없을까?
긴 단어들이라면?


```python
V = set(text1)
long_words = [w for w in V if len(w) > 15]
sorted(long_words)
```

단어의 길이들을 살펴보는 것도 텍스트에 대한 유용한 정보를 줌


```python
[len(w) for w in text1]
```


```python
fdist = FreqDist(len(w) for w in text1)
print(fdist)
```


```python
fdist
```

가장 많은 글자 수는 몇개?


```python
fdist.most_common()
```


```python
fdist.freq(3)
```

(실습) 한글 의안의 경우는? 
